{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1768142840296,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "RIwd22AhSqeS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1768142840433,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "kwbRllaL_f38"
   },
   "outputs": [],
   "source": [
    "def mount_drive ():\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142840436,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "9QgMpHGoA1KE"
   },
   "outputs": [],
   "source": [
    "def list_drive_datasets(drive_dir=\"/content/drive/MyDrive/colab_datasets\"):\n",
    "    \"\"\"\n",
    "    List all files in a Google Drive folder.\n",
    "    \"\"\"\n",
    "    os.makedirs(drive_dir, exist_ok=True)\n",
    "    return [f for f in os.listdir(drive_dir) if os.path.isfile(os.path.join(drive_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142840437,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "JUhuq6O3Czd-"
   },
   "outputs": [],
   "source": [
    "def upload_dataset_to_drive(drive_dir=\"/content/drive/MyDrive/colab_datasets\"):\n",
    "    \"\"\"\n",
    "    Upload a new dataset to Google Drive permanently, with unique ID.\n",
    "    \"\"\"\n",
    "    os.makedirs(drive_dir, exist_ok=True)\n",
    "\n",
    "    uploaded = files.upload()\n",
    "    original_name = list(uploaded.keys())[0]\n",
    "\n",
    "    name, ext = original_name.rsplit(\".\", 1)\n",
    "    unique_id = str(uuid.uuid4())[:8]\n",
    "    new_file_name = f\"{name}_{unique_id}.{ext}\"\n",
    "\n",
    "    drive_path = os.path.join(drive_dir, new_file_name)\n",
    "\n",
    "    with open(drive_path, \"wb\") as f:\n",
    "        f.write(uploaded[original_name])\n",
    "\n",
    "    print(f\" Uploaded to Drive: {new_file_name}\")\n",
    "    return drive_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142840438,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "YCKIVm7HDEqI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def restore_file_to_colab(drive_file_path, target_colab_dir=\"/content/datasets\"):\n",
    "    \"\"\"\n",
    "    Restore a file from Drive into Colab workspace.\n",
    "    Returns metadata for validation or Spark pipeline.\n",
    "    \"\"\"\n",
    "    os.makedirs(target_colab_dir, exist_ok=True)\n",
    "    file_name = os.path.basename(drive_file_path)\n",
    "    colab_path = os.path.join(target_colab_dir, file_name)\n",
    "\n",
    "    shutil.copy(drive_file_path, colab_path)\n",
    "\n",
    "    metadata = {\n",
    "        \"file_name\": file_name,\n",
    "        \"file_path\": colab_path,\n",
    "        \"drive_path\": drive_file_path,\n",
    "        \"file_extension\": file_name.split(\".\")[-1].lower(),\n",
    "        \"file_size_bytes\": os.path.getsize(colab_path)\n",
    "    }\n",
    "\n",
    "    print(f\" File restored to Colab: {colab_path}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1768142840439,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "Lz6nV97-T84Z"
   },
   "outputs": [],
   "source": [
    "def persist_file_to_drive(colab_file_path, drive_dir=\"/content/drive/MyDrive/colab_datasets\"):\n",
    "    \"\"\"\n",
    "    Save any file from Colab to Drive permanently.\n",
    "    Returns metadata of the saved file.\n",
    "    \"\"\"\n",
    "    os.makedirs(drive_dir, exist_ok=True)\n",
    "\n",
    "    file_name = os.path.basename(colab_file_path)\n",
    "    name, ext = file_name.rsplit(\".\", 1)\n",
    "    unique_id = str(uuid.uuid4())[:8]\n",
    "    drive_file_name = f\"{name}_{unique_id}.{ext}\"\n",
    "    drive_path = os.path.join(drive_dir, drive_file_name)\n",
    "\n",
    "    shutil.copy(colab_file_path, drive_path)\n",
    "\n",
    "    metadata = {\n",
    "        \"original_colab_path\": colab_file_path,\n",
    "        \"drive_path\": drive_path,\n",
    "        \"file_name\": drive_file_name,\n",
    "        \"file_extension\": ext.lower(),\n",
    "        \"file_size_bytes\": os.path.getsize(drive_path)\n",
    "    }\n",
    "\n",
    "    print(f\" File persisted to Drive: {drive_path}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768142840599,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "mtsrIZ_nQo4S"
   },
   "outputs": [],
   "source": [
    "def storage_ui_pipeline(colab_dir=\"/content/datasets\", drive_dir=\"/content/drive/MyDrive/colab_datasets\"):\n",
    "    \"\"\"\n",
    "    Interactive storage pipeline:\n",
    "    1. Ask user to use existing dataset or upload new\n",
    "    2. Restore dataset into Colab\n",
    "    3. Return metadata for validation and Spark pipeline\n",
    "    \"\"\"\n",
    "    mount_drive()\n",
    "\n",
    "    # List existing files\n",
    "    existing_files = list_drive_datasets(drive_dir)\n",
    "\n",
    "    # Interactive selection\n",
    "    if existing_files:\n",
    "        print(\"Existing datasets in Google Drive:\")\n",
    "        for i, f in enumerate(existing_files, 1):\n",
    "            print(f\"{i} - {f}\")\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1 - Use existing dataset\")\n",
    "        print(\"2 - Upload new dataset\")\n",
    "        choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    else:\n",
    "        print(\"No existing datasets found. You must upload a new dataset.\")\n",
    "        choice = \"2\"\n",
    "\n",
    "    # Existing dataset\n",
    "    if choice == \"1\":\n",
    "        idx = int(input(\"Enter dataset number: \")) - 1\n",
    "        drive_path = os.path.join(drive_dir, existing_files[idx])\n",
    "    # Upload new dataset\n",
    "    else:\n",
    "        drive_path = upload_dataset_to_drive(drive_dir)\n",
    "        print(\" Dataset saved permanently in Drive.\")\n",
    "\n",
    "    # Restore to Colab\n",
    "    dataset_meta = restore_file_to_colab(drive_path, colab_dir)\n",
    "\n",
    "    return dataset_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142840600,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "eBBj7ntWtDTp"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_dataset(metadata):\n",
    "    \"\"\"\n",
    "    Validate dataset type and detect Spark read strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    allowed_types = [\"csv\", \"json\", \"txt\", \"pdf\"]\n",
    "    ext = metadata[\"file_extension\"]\n",
    "\n",
    "    if ext not in allowed_types:\n",
    "        raise ValueError(f\"Unsupported file type: .{ext}\")\n",
    "\n",
    "    spark_read_config = {\n",
    "        \"format\": None,\n",
    "        \"options\": {}\n",
    "    }\n",
    "\n",
    "    if ext == \"csv\":\n",
    "        spark_read_config[\"format\"] = \"csv\"\n",
    "        spark_read_config[\"options\"] = {\n",
    "            \"header\": \"true\",\n",
    "            \"inferSchema\": \"true\"\n",
    "        }\n",
    "\n",
    "    elif ext == \"json\":\n",
    "        spark_read_config[\"format\"] = \"json\"\n",
    "\n",
    "    elif ext == \"txt\":\n",
    "        spark_read_config[\"format\"] = \"text\"\n",
    "\n",
    "    elif ext == \"pdf\":\n",
    "        spark_read_config[\"format\"] = \"pdf\"\n",
    "        print(\" PDF detected: requires preprocessing before Spark ML\")\n",
    "\n",
    "    print(\" Dataset validated\")\n",
    "    print(spark_read_config)\n",
    "\n",
    "    return {\n",
    "        \"valid\": True,\n",
    "        \"spark_read_config\": spark_read_config\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1768142842036,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "w7xmfZfSselk"
   },
   "outputs": [],
   "source": [
    "def select_job_and_subtasks_numbered():\n",
    "    \"\"\"\n",
    "    Ask user to choose main job type (Descriptive / ML)\n",
    "    and then select subtasks using numbers (e.g., 1,2,7).\n",
    "    Returns full job configuration object.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Main Job Type ---\n",
    "    print(\"Select Main Job Type:\")\n",
    "    print(\"1 - Descriptive Statistics\")\n",
    "    print(\"2 - Machine Learning\")\n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "\n",
    "    config = {}\n",
    "\n",
    "    # --- Descriptive Statistics ---\n",
    "    if choice == \"1\":\n",
    "        config[\"mode\"] = \"descriptive\"\n",
    "        descriptive_tasks = [\n",
    "            \"row_count\",\n",
    "            \"column_count\",\n",
    "            \"data_types\",\n",
    "            \"min_max_mean\",\n",
    "            \"null_percentage\",\n",
    "            \"unique_counts\"\n",
    "        ]\n",
    "        print(\"\\nSelect Descriptive Subtasks (comma separated numbers, e.g., 1,3,5):\")\n",
    "        for i, task in enumerate(descriptive_tasks, start=1):\n",
    "            print(f\"{i} - {task}\")\n",
    "        selections = input(\"Enter your choices: \").split(\",\")\n",
    "        config[\"tasks\"] = [descriptive_tasks[int(s.strip())-1] for s in selections]\n",
    "\n",
    "    # --- Machine Learning ---\n",
    "    elif choice == \"2\":\n",
    "        config[\"mode\"] = \"ml\"\n",
    "        ml_tasks = [\"regression\", \"kmeans\", \"fpgrowth\", \"timeseries\"]\n",
    "        print(\"\\nSelect ML Subtasks (comma separated numbers, e.g., 1,2):\")\n",
    "        for i, task in enumerate(ml_tasks, start=1):\n",
    "            print(f\"{i} - {task}\")\n",
    "        selections = input(\"Enter your choices: \").split(\",\")\n",
    "        config[\"tasks\"] = [ml_tasks[int(s.strip())-1] for s in selections]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid selection. Choose 1 or 2.\")\n",
    "\n",
    "    print(\"\\n Job Configuration Complete\")\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1768142842703,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "PhW1pBrOyY7_"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, countDistinct, isnan, when,\n",
    "    min, max, mean, sum as spark_sum\n",
    ")\n",
    "\n",
    "def run_descriptive_subtasks(df, selected_tasks):\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    # ---------- Basic metadata ----------\n",
    "    if \"row_count\" in selected_tasks:\n",
    "        output[\"row_count\"] = df.count()\n",
    "\n",
    "    if \"column_count\" in selected_tasks:\n",
    "        output[\"column_count\"] = len(df.columns)\n",
    "\n",
    "    if \"data_types\" in selected_tasks:\n",
    "        output[\"data_types\"] = {\n",
    "            f.name: str(f.dataType) for f in df.schema.fields\n",
    "        }\n",
    "\n",
    "    # ---------- Numeric column detection ----------\n",
    "    numeric_types = {\"int\", \"double\", \"float\", \"long\"}\n",
    "    numeric_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if f.dataType.simpleString() in numeric_types\n",
    "    ]\n",
    "\n",
    "    # ---------- Min / Max / Mean ----------\n",
    "    if \"min_max_mean\" in selected_tasks and numeric_cols:\n",
    "        agg_exprs = []\n",
    "        for c in numeric_cols:\n",
    "            agg_exprs.extend([\n",
    "                min(col(c)).alias(f\"{c}__min\"),\n",
    "                max(col(c)).alias(f\"{c}__max\"),\n",
    "                mean(col(c)).alias(f\"{c}__mean\")\n",
    "            ])\n",
    "\n",
    "        row = df.agg(*agg_exprs).collect()[0].asDict()\n",
    "\n",
    "        stats = {}\n",
    "        for c in numeric_cols:\n",
    "            stats[c] = {\n",
    "                \"min\": row.get(f\"{c}__min\"),\n",
    "                \"max\": row.get(f\"{c}__max\"),\n",
    "                \"mean\": row.get(f\"{c}__mean\")\n",
    "            }\n",
    "\n",
    "        output[\"min_max_mean\"] = stats\n",
    "\n",
    "    # ---------- Null percentage ----------\n",
    "    if \"null_percentage\" in selected_tasks:\n",
    "        total_rows = df.count()\n",
    "\n",
    "        null_exprs = []\n",
    "        for c in df.columns:\n",
    "            if c in numeric_cols:\n",
    "                null_exprs.append(\n",
    "                    spark_sum(\n",
    "                        when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)\n",
    "                    ).alias(c)\n",
    "                )\n",
    "            else:\n",
    "                null_exprs.append(\n",
    "                    spark_sum(\n",
    "                        when(col(c).isNull(), 1).otherwise(0)\n",
    "                    ).alias(c)\n",
    "                )\n",
    "\n",
    "        null_counts = df.agg(*null_exprs).collect()[0].asDict()\n",
    "\n",
    "        output[\"null_percentage\"] = {\n",
    "            c: round((null_counts[c] / total_rows) * 100, 2)\n",
    "            for c in df.columns\n",
    "        }\n",
    "\n",
    "    # ---------- Unique counts ----------\n",
    "    if \"unique_counts\" in selected_tasks:\n",
    "        unique_exprs = [\n",
    "            countDistinct(col(c)).alias(c) for c in df.columns\n",
    "        ]\n",
    "\n",
    "        unique_counts = df.agg(*unique_exprs).collect()[0].asDict()\n",
    "        output[\"unique_counts\"] = unique_counts\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1768142842704,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "YKAPjG6EyqeP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "def run_kmeans(df, k=3):\n",
    "    numeric_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() in ['int','double']]\n",
    "    if not numeric_cols:\n",
    "        return \"Skipped: no numeric columns\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "    data = assembler.transform(df)\n",
    "\n",
    "    model = KMeans(k=k, seed=1).fit(data)\n",
    "    return model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1768142842705,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "KZge1QxeyxmG"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "def run_regression(df):\n",
    "    numeric_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() in ['int','double']]\n",
    "    if len(numeric_cols) < 2:\n",
    "        return \"Skipped: not enough numeric columns\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=numeric_cols[:-1], outputCol=\"features\")\n",
    "    data = assembler.transform(df).withColumnRenamed(numeric_cols[-1], \"label\")\n",
    "\n",
    "    model = LinearRegression(featuresCol=\"features\", labelCol=\"label\").fit(data)\n",
    "    return model.coefficients.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142842707,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "wL-fp7MMy2xP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "def run_fpgrowth(df, min_support=0.5):\n",
    "    if \"items\" not in df.columns:\n",
    "        return \"Skipped: 'items' column required\"\n",
    "\n",
    "    fp = FPGrowth(itemsCol=\"items\", minSupport=min_support, minConfidence=0.6)\n",
    "    model = fp.fit(df)\n",
    "    return model.freqItemsets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142842708,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "H2VViKU6y6Io"
   },
   "outputs": [],
   "source": [
    "def run_timeseries(df, timestamp_col=\"timestamp\", freq=\"day\"):\n",
    "    if timestamp_col not in df.columns:\n",
    "        return \"Skipped: timestamp column required\"\n",
    "\n",
    "    # Basic example: count per day\n",
    "    from pyspark.sql.functions import to_date, col\n",
    "    ts_df = df.withColumn(\"date\", to_date(col(timestamp_col)))\n",
    "    result = ts_df.groupBy(\"date\").count().collect()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1768142842708,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "KppY5D2Ay9V3"
   },
   "outputs": [],
   "source": [
    "def run_ml_subtasks(df, selected_tasks):\n",
    "    \"\"\"\n",
    "    Executes ML subtasks modularly.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for task in selected_tasks:\n",
    "        if task == \"kmeans\":\n",
    "            results[\"kmeans\"] = run_kmeans(df)\n",
    "        elif task == \"regression\":\n",
    "            results[\"regression\"] = run_regression(df)\n",
    "        elif task == \"fpgrowth\":\n",
    "            results[\"fpgrowth\"] = run_fpgrowth(df)\n",
    "        elif task == \"timeseries\":\n",
    "            results[\"timeseries\"] = run_timeseries(df)\n",
    "        else:\n",
    "            results[task] = \"Unknown ML task\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1768142843130,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "j567iJy8tlNw"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_spark_session(nodes=1):\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "            .appName(f\"JobExecutor_{nodes}_nodes\") \\\n",
    "            .master(f\"local[{nodes}]\") \\\n",
    "            .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142843131,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "rEjnYGQs1CCt"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, min, max, mean\n",
    "\n",
    "def run_pipeline_cached(dataset_meta, spark_read_config, job_config, node_list=[1,2,4,8]):\n",
    "    \"\"\"\n",
    "    Executes the main job on multiple node counts with caching for improved performance.\n",
    "    Returns job outputs and performance metrics.\n",
    "    \"\"\"\n",
    "    perf_records = []\n",
    "    job_outputs = {}\n",
    "\n",
    "    for nodes in node_list:\n",
    "        print(f\"\\n--- Running job on {nodes} node(s) ---\")\n",
    "        spark =create_spark_session(nodes)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Load Dataset\n",
    "        # -----------------------------\n",
    "        reader = spark.read.format(spark_read_config[\"format\"])\n",
    "        for k, v in spark_read_config.get(\"options\", {}).items():\n",
    "            reader = reader.option(k, v)\n",
    "        df = reader.load(dataset_meta[\"file_path\"])\n",
    "\n",
    "        # -----------------------------\n",
    "        # Cache dataset in memory\n",
    "        # -----------------------------\n",
    "        df.cache()\n",
    "        df.count()  # Force Spark to load data into memory\n",
    "        print(\" Dataset cached in memory\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Run Job\n",
    "        # -----------------------------\n",
    "        start_time = time.time()\n",
    "        if job_config[\"mode\"] == \"descriptive\":\n",
    "            output = run_descriptive_subtasks(df, job_config[\"tasks\"])\n",
    "        else:\n",
    "            output = run_ml_subtasks(df, job_config[\"tasks\"])\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Save outputs & metrics\n",
    "        job_outputs[f\"{nodes}_nodes\"] = output\n",
    "        perf_records.append({\"nodes\": nodes, \"time_sec\": duration})\n",
    "        print(f\"Execution time: {duration:.2f} sec\")\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "    # Compute speedup and efficiency\n",
    "    base_time = perf_records[0][\"time_sec\"]\n",
    "    for r in perf_records:\n",
    "        r[\"speedup\"] = round(base_time / r[\"time_sec\"], 2)\n",
    "        r[\"efficiency\"] = round(r[\"speedup\"] / r[\"nodes\"], 2)\n",
    "\n",
    "    perf_table = pd.DataFrame(perf_records)\n",
    "    return job_outputs, perf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142843132,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "UuI5-wRRVB4M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def persist_pipeline_results(\n",
    "    job_outputs: dict,\n",
    "    perf_table: pd.DataFrame,\n",
    "    show_node: str,\n",
    "    drive_dir: str = \"/content/drive/MyDrive/cloud_results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Display performance metrics as a table\n",
    "    2. Show output of ONE selected node\n",
    "    3. Save ALL node outputs + performance table to Drive\n",
    "    4. Return saved paths & metadata\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(perf_table, pd.DataFrame):\n",
    "        raise TypeError(\"perf_table must be a pandas DataFrame\")\n",
    "\n",
    "    if show_node not in job_outputs:\n",
    "        raise ValueError(f\"Node '{show_node}' not found in job_outputs\")\n",
    "\n",
    "    os.makedirs(drive_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    unique_id = str(uuid.uuid4())[:8]\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_folder = os.path.join(drive_dir, f\"run_{timestamp}_{unique_id}\")\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    print(\"Performance Metrics\")\n",
    "    display(perf_table)\n",
    "\n",
    "    perf_file = os.path.join(run_folder, \"performance_metrics.csv\")\n",
    "    perf_table.to_csv(perf_file, index=False)\n",
    "\n",
    "\n",
    "    print(f\"Result from node: {show_node}\")\n",
    "    print(job_outputs[show_node])\n",
    "\n",
    "\n",
    "    node_output_paths = {}\n",
    "\n",
    "    for node, output in job_outputs.items():\n",
    "        node_file = os.path.join(run_folder, f\"node_{node}_output.txt\")\n",
    "        with open(node_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(output))\n",
    "        node_output_paths[node] = node_file\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        \"run_folder\": run_folder,\n",
    "        \"performance_table_path\": perf_file,\n",
    "        \"node_output_paths\": node_output_paths,\n",
    "        \"shown_node\": show_node,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"run_id\": unique_id\n",
    "    }\n",
    "\n",
    "    print(f\"\\nAll results saved successfully\")\n",
    "    print(f\"Drive path: {run_folder}\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768142843133,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "As-kpASi4exZ"
   },
   "outputs": [],
   "source": [
    "def pipeline_ui_app():\n",
    "    \"\"\"\n",
    "    Orchestrates the full pipeline with UI for each phase,\n",
    "    invoking existing functions without re-implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    #  Upload Dataset\n",
    "    # -----------------------------\n",
    "    print(\"=== Phase 1: Upload Dataset ===\")\n",
    "    dataset_meta = storage_ui_pipeline()  # existing upload function\n",
    "\n",
    "    # -----------------------------\n",
    "    #  Validate Dataset\n",
    "    # -----------------------------\n",
    "    print(\"\\n=== Phase 2: Validate Dataset ===\")\n",
    "    validation = validate_dataset(dataset_meta)  # existing validation function\n",
    "    spark_read_config = validation[\"spark_read_config\"]\n",
    "\n",
    "    # -----------------------------\n",
    "    #  Select Job & Subtasks\n",
    "    # -----------------------------\n",
    "    print(\"\\n=== Phase 3: Select Job and Subtasks ===\")\n",
    "    job_config = select_job_and_subtasks_numbered()  # existing selection function\n",
    "\n",
    "    # -----------------------------\n",
    "    #  Run Pipeline\n",
    "    # -----------------------------\n",
    "    print(\"\\n=== Phase 4: Execute Pipeline on Nodes ===\")\n",
    "    job_outputs, perf_table = run_pipeline_cached(dataset_meta, spark_read_config, job_config)  # existing pipeline\n",
    "\n",
    "    # -----------------------------\n",
    "    #  Display & Save Results\n",
    "    # -----------------------------\n",
    "    print(\"\\n=== Phase 5: Display and Save Results ===\")\n",
    "    persist_pipeline_results(job_outputs, perf_table, show_node=\"1_nodes\")  # existing display/save function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 63807,
     "status": "ok",
     "timestamp": 1768143066982,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "SpDMG2pC4mzC",
    "outputId": "14dafb71-b12e-4eca-dcab-834554fb8714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 1: Upload Dataset ===\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Existing datasets in Google Drive:\n",
      "1 - customers-100000 (1)_8d8a3627.csv\n",
      "\n",
      "Options:\n",
      "1 - Use existing dataset\n",
      "2 - Upload new dataset\n",
      "Enter choice (1 or 2): 1\n",
      "Enter dataset number: 1\n",
      " File restored to Colab: /content/datasets/customers-100000 (1)_8d8a3627.csv\n",
      "\n",
      "=== Phase 2: Validate Dataset ===\n",
      " Dataset validated\n",
      "{'format': 'csv', 'options': {'header': 'true', 'inferSchema': 'true'}}\n",
      "\n",
      "=== Phase 3: Select Job and Subtasks ===\n",
      "Select Main Job Type:\n",
      "1 - Descriptive Statistics\n",
      "2 - Machine Learning\n",
      "Enter choice (1 or 2): 2\n",
      "\n",
      "Select ML Subtasks (comma separated numbers, e.g., 1,2):\n",
      "1 - regression\n",
      "2 - kmeans\n",
      "3 - fpgrowth\n",
      "4 - timeseries\n",
      "Enter your choices: 2\n",
      "\n",
      " Job Configuration Complete\n",
      "\n",
      "=== Phase 4: Execute Pipeline on Nodes ===\n",
      "\n",
      "--- Running job on 1 node(s) ---\n",
      " Dataset cached in memory\n",
      "Execution time: 12.16 sec\n",
      "\n",
      "--- Running job on 2 node(s) ---\n",
      " Dataset cached in memory\n",
      "Execution time: 8.78 sec\n",
      "\n",
      "--- Running job on 4 node(s) ---\n",
      " Dataset cached in memory\n",
      "Execution time: 8.63 sec\n",
      "\n",
      "--- Running job on 8 node(s) ---\n",
      " Dataset cached in memory\n",
      "Execution time: 8.21 sec\n",
      "\n",
      "=== Phase 5: Display and Save Results ===\n",
      "Performance Metrics\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"pipeline_ui_app()\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"nodes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          8,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.827719297974749,\n        \"min\": 8.21122694015503,\n        \"max\": 12.163590908050537,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          8.780622005462646,\n          8.21122694015503,\n          12.163590908050537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speedup\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21679483388678797,\n        \"min\": 1.0,\n        \"max\": 1.48,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.39,\n          1.48,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"efficiency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3646459470043053,\n        \"min\": 0.18,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.69,\n          0.18,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-1aa61872-b48d-4afc-a94b-37928c07579e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>speedup</th>\n",
       "      <th>efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.163591</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.780622</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>8.628088</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8.211227</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1aa61872-b48d-4afc-a94b-37928c07579e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1aa61872-b48d-4afc-a94b-37928c07579e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1aa61872-b48d-4afc-a94b-37928c07579e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-dbe13f60-7f62-49b1-bfa5-e8210fe579a4\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dbe13f60-7f62-49b1-bfa5-e8210fe579a4')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-dbe13f60-7f62-49b1-bfa5-e8210fe579a4 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   nodes   time_sec  speedup  efficiency\n",
       "0      1  12.163591     1.00        1.00\n",
       "1      2   8.780622     1.39        0.69\n",
       "2      4   8.628088     1.41        0.35\n",
       "3      8   8.211227     1.48        0.18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from node: 1_nodes\n",
      "{'kmeans': [array([49969.5]), array([16651.5]), array([83318.5])]}\n",
      "\n",
      "All results saved successfully\n",
      "Drive path: /content/drive/MyDrive/cloud_results/run_20260111_145106_42136076\n"
     ]
    }
   ],
   "source": [
    "pipeline_ui_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1768142982890,
     "user": {
      "displayName": "Ahmed Habeeb",
      "userId": "01244333181829436404"
     },
     "user_tz": -120
    },
    "id": "Lqpv9C1-Pd7V",
    "outputId": "8833215a-a97d-4cd0-a48a-441a6a89a4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical cores (threads): 2\n",
      "Physical cores: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Number of logical processors (threads)\n",
    "logical_cores = os.cpu_count()\n",
    "print(\"Logical cores (threads):\", logical_cores)\n",
    "\n",
    "# Number of physical cores\n",
    "try:\n",
    "    import psutil\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    print(\"Physical cores:\", physical_cores)\n",
    "except ImportError:\n",
    "    print(\"psutil not installed, physical cores info not available\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPu3nRyjO3xlQ2m1PB2S3yb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
